{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88857066-848f-4db4-97d4-dfbbd9ba2c44",
   "metadata": {},
   "source": [
    "# Deep Q Network\n",
    "\n",
    "> Credits: This work is based on a number of sources.  But I would like to give credit to Dr Mike Allen (PenCHORD, UoE) who introduced me to the method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff18db-b8d2-41b4-a29d-fc1fb59a93cb",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "### gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a4172d-8a23-4bb6-8bf9-363ee015e0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.21.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2b2582-856c-49e9-badb-64bad0c5df99",
   "metadata": {},
   "source": [
    "### pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd612c2e-bbe3-4e56-9ba6-1f950985b516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdf808e-54b6-48ca-8bc4-76f9d5623c7a",
   "metadata": {},
   "source": [
    "### standard\n",
    "\n",
    "In addition to the standard data science imports this implementation will also make use of a `dequeue`. This will act as the DQN's memory buffer (used during sampling).  When this is full one sample will enter and the oldest will leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbd70ba7-802b-497d-8d78-8253c9fbeca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2528df7-e2cc-4bee-9122-061d46b4e0ce",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a8af23a-7873-4ca9-96aa-704b25fd39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "################ EXPLORATION / EXPLOITATION SETTINGS ###########################\n",
    "\n",
    "# default value for maximum epsilon to anneal\n",
    "DEFAULT_EPSILON_MAX = 1.0\n",
    "\n",
    "EXPLORATION_DECAY = 0.99\n",
    "EXPLORATION_MIN = 0.1\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8e4dc1c-e782-403a-a899-ff0a2688b74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### OPTIMISATION ##############################################\n",
    "\n",
    "# discount rate of future rewards\n",
    "GAMMA = 0.99\n",
    "\n",
    "# replay buffer size\n",
    "DEFAULT_BUFFER_SIZE = 10_000\n",
    "\n",
    "# mini batch size for training\n",
    "DEFAULT_BATCH_SIZE = 5\n",
    "\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f830bf23-7306-4a82-8aac-7bfa574310f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### DQN AGENT ############################################\n",
    "\n",
    "# default learning rate\n",
    "DEFAULT_LR = 0.01\n",
    "\n",
    "# default neurons in middle layer of DQN\n",
    "DEFAULT_N_NEURONS_ML = 24\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61583dcc-7143-4503-94ee-843f7da2a802",
   "metadata": {},
   "source": [
    "## Agent class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "264eee27-d41b-42da-9d0e-eca8b980e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    '''\n",
    "    \n",
    "    In a Double Deep Q Network this network architecture is used for both the \n",
    "    policy network and the target networks.  \n",
    "    \n",
    "    Policy network chooses the action.\n",
    "    Target network returns the Q.\n",
    "    \n",
    "    The seperation is done for stability.  Two consecutive states will be \n",
    "    close to each other and an update to a single network will change\n",
    "    nearby values of Q.\n",
    "    \n",
    "    Implementation Notes:\n",
    "    -----\n",
    "    Objective = MSE\n",
    "    Optimizer = Adam\n",
    "    \n",
    "    The architecture used is a simple 3 layer network.\n",
    "        \n",
    "    '''\n",
    "    def __init__(self, observation_space, action_space,\n",
    "                 n_neurons_middle=DEFAULT_N_NEURONS_ML,\n",
    "                 epsilon_init=DEFAULT_EPSILON_MAX, random_seed=None,\n",
    "                 learning_rate=DEFAULT_LR):\n",
    "        '''\n",
    "        Constructor for DQN agent. Set parameters and create network \n",
    "        architecture.\n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        observation_space: int\n",
    "            Size of the observation space\n",
    "            \n",
    "        action_space: int\n",
    "            The number of actions e.g. 5 that an agent could choose.\n",
    "            \n",
    "        n_neurons_middle: int, optional (default=DEFAULT_N_NEURONS_ML)\n",
    "            The number of neurons int he middle layer of the network.\n",
    "        \n",
    "        epsilon_init: float, optional (default=DEFAULT_EPSILON_MAX)\n",
    "            The initial value of epsilon.  This value epsilon \n",
    "            controls the exploration rate for the agent.\n",
    "            \n",
    "        random_seed: int, optional (default=None)\n",
    "            controls the exploration sampling. set to an int for repeatable\n",
    "            exploration (does not control SGD!)\n",
    "            \n",
    "        learning_rate: float, optional (default=DEFAULT_LR)\n",
    "            LR used by the Adam optimiser.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        # Inherit parent (nn.module) methods using super init\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        # exploration / exploitation settings\n",
    "        self.epsilon = epsilon_init\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "        \n",
    "        # network architecture\n",
    "        self.net = nn.Sequential(\n",
    "            # Linear(no. in features, no. out features)\n",
    "            nn.Linear(observation_space, n_neurons_middle),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(n_neurons_middle, n_neurons_middle),\n",
    "            nn.ReLu(),\n",
    "            nn.Linear(n_neurons_middle, action_space))\n",
    "        \n",
    "        \n",
    "        self.objective = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.net.parameters, lr=learning_rate)\n",
    "        \n",
    "        \n",
    "    def take_action(self, state):\n",
    "        '''\n",
    "        Choose an action from the action space.\n",
    "        \n",
    "        Epsilon greedy implementation.  \n",
    "        \n",
    "        Parameters:\n",
    "        ----------\n",
    "        state: np.ndarray\n",
    "            observations of the environment.\n",
    "        \n",
    "        Returns:\n",
    "        -------\n",
    "        int\n",
    "            The zero indexed action that has been chosen.\n",
    "        '''\n",
    "        if self.rng.uniform() < self.epsilon:\n",
    "            # take range action\n",
    "            action = rng.integers(0, action_space)\n",
    "        else:\n",
    "            # predict Q\n",
    "            q_values = self.net(torch.FloatTensor(state))\n",
    "            # max Q\n",
    "            action = np.argmax(q_values.detach().numpy()[0])\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Foward pass through the net. Returns a prediction.\n",
    "        '''\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f56fd1-b703-42c0-81f3-eecd22bbcb06",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96ffff11-af34-4a39-a72b-7d330ded3b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    '''\n",
    "    Using a replay buffer helps to stablise the optimisation.  \n",
    "    Samples appear to be Independent Identically Distributed (IID).\n",
    "    \n",
    "    Implementation notes.\n",
    "    --------------------\n",
    "    Here the buffer in implemented as a `collections.deque`.  When full the \n",
    "    oldest observations are pushed out.\n",
    "    '''\n",
    "    def __init__(self, buffer_size=DEFAULT_BUFFER_SIZE, random_seed=None):\n",
    "        self.buffer = deque(maxlen=buffer_size)\n",
    "        self.rng = np.random.default_rng(random_seed)\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    @property\n",
    "    def size(self):\n",
    "        '''\n",
    "        No. observations in the replay buffer. Returns int.\n",
    "        '''\n",
    "        return len(self.buffer)\n",
    "        \n",
    "    def sample(self, size):\n",
    "        '''\n",
    "        uniform sample of observations from the replay buffer\n",
    "        '''\n",
    "        return rng.choice(self.deque, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f52d6ce6-f0c0-4ddd-bea7-f0eb735e4cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(policy_net, target_net, replay_buffer, \n",
    "             batch_size=DEFAULT_BATCH_SIZE):\n",
    "    '''\n",
    "    Parameters:\n",
    "    -----------\n",
    "    policy_net: DQN\n",
    "        Policy network to predict best action (best Q).\n",
    "    target_net: DQN\n",
    "        Target network to provide target of Q for the selected next action.\n",
    "    replay_buffer: ReplayBuffer \n",
    "        A memory buffer from which experience is uniformly sampled \n",
    "    batch_size: int\n",
    "        Mini batch size for training.  \n",
    "        \n",
    "    '''    \n",
    "    # return if replay buffer size if less than \n",
    "    if replay_buffer.size < batch_size:\n",
    "        return\n",
    "    \n",
    "    # Reduce exploration rate (exploration rate is stored in policy net)\n",
    "    policy_net.epsilon *= EXPLORATION_DECAY\n",
    "    policy_net.epsilon = max(EXPLORATION_MIN, policy_net.epsilon)\n",
    "    \n",
    "    # sample a mini batch from the replay buffer\n",
    "    training_batches = replay_buffer.sample(size=batch_size)\n",
    "    \n",
    "    # loop through the mini batch\n",
    "    for state, action, reward, state_next, terminal in training_batches:\n",
    "        \n",
    "        state_action_values = policy_net(torch.FloatTensor(state))\n",
    "                                    \n",
    "        # Get Q from target network in order to do policy net update\n",
    "        \n",
    "        # current state Q predicted by policy net - call this \"expected\"\n",
    "        expected_state_action_values = \\\n",
    "            policy_net(torch.FloatTensor(state)).detach()\n",
    "        \n",
    "        if not terminal:\n",
    "            ###################### POLICY NET ##################################\n",
    "            # get best action for the next state from policy network\n",
    "            policy_next_state_action_values = \\\n",
    "                policy_net(torch.FloatTensor(state_next)).detach()\n",
    "            \n",
    "            best_action = np.argmax(policy_next_state_action_values[0].numpy())\n",
    "            ####################################################################\n",
    "            \n",
    "            ##################### TARGET NET ###################################\n",
    "            # predict Q with target network and use best action from policy net\n",
    "            next_state_action_values = \\\n",
    "                target_net(torch.FloatTensor(state_next)).detach()\n",
    "            \n",
    "            best_next_q = next_state_action_values[0][best_action].numpy()\n",
    "            ####################################################################\n",
    "            \n",
    "            ################# UPDATE Q WITH BELLMAN EQ. ########################\n",
    "            best_next_q = reward + (GAMMA * best_next_q)\n",
    "            expected_state_action_values[0][action] = updated_q\n",
    "            ####################################################################\n",
    "            \n",
    "        else:\n",
    "            # Set Q for all actions to reward (-1)\n",
    "            expected_state_action_values[0] = reward\n",
    "            \n",
    "        # Reset net gradients\n",
    "        policy_net.optimizer.zero_grad()  \n",
    "        # calculate loss\n",
    "        loss_v = nn.MSELoss()(state_action_values, expected_state_action_values)\n",
    "        # Backpropogate loss\n",
    "        loss_v.backward()\n",
    "        # Update network gradients\n",
    "        policy_net.optimizer.step()  \n",
    "        \n",
    "    return    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d72d7-c7af-4fa1-a3c8-df788c931474",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
